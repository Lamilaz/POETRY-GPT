# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sRe8CNleXwVr6FeLxBXC02ytb-vI4H4N
"""

# IMPORTS
import tokenizers
import wandb
import torch
import torch.nn as nn
from torch.nn import functional as F
from torch.utils.data import Dataset, DataLoader
from tokenizers import Tokenizer
from tokenizers.models import BPE
from tokenizers.trainers import BpeTrainer
from tokenizers.pre_tokenizers import Whitespace, Sequence, Split, Digits
from tokenizers import normalizers
from datasets import load_dataset
import random
import numpy as np


# --- 1. SETUP GLOBAL ---
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

# HYPERPARAMETERS
dropout = 0.2
d_model = 512
vocab_size = 30000
n_heads = 8
n_main_layers = 6
block_size = 256
batch_size = 32
lr = 3e-3
max_iters = 5000
eval_every = 100
eval_iters = 50
save_every = 200
checkpoint = "lamilaz.pt"

# LOAD TOKENIZER
tokenizer = Tokenizer.from_file("tokenizer.json")
vocab_size = tokenizer.get_vocab_size()

wandb.init(
    project="nano-gpt",
    config={
        "model": "lamilaz",
        "batch_size": batch_size,
        "block_size": block_size,
        "d_model": d_model,
        "main_layers": n_main_layers,
        "n_heads": n_heads,
        "dropout": dropout,
        "learning_rate": lr,
        "vocab_size": vocab_size
    }
)

# FONCTIONS GLOBALES
def encode(s):
    if hasattr(tokenizer, 'encode'):
        return tokenizer.encode(s).ids
    return []

def decode(ids):
    if hasattr(tokenizer, 'decode'):
        return tokenizer.decode(ids)
    return ""


# --- 2. DATASET AVEC STREAMING (ZÉRO RAM) ---
class StreamingTextDataset(Dataset):
    """Dataset qui charge les textes à la demande depuis le stream"""

    def __init__(self, stream_dataset, tokenizer, block_size, max_samples=100000):
        self.tokenizer = tokenizer
        self.block_size = block_size
        self.max_samples = max_samples

        # On ne charge QUE les indices, pas les textes
        print(f"Préparation du cache d'indices (max {max_samples} samples)...")
        self.samples = []  # Liste de (text, start_idx) pour chaque sample

        sample_count = 0
        text_count = 0

        for item in stream_dataset:
            if sample_count >= max_samples:
                break

            text = item["text"]
            if not text or len(text.strip()) == 0:
                continue

            # Encoder UNE SEULE FOIS pour compter
            tokens = encode(text)
            num_samples = max(1, len(tokens) - block_size)

            # Stocker le texte ET les positions possibles
            for start_idx in range(num_samples):
                if sample_count >= max_samples:
                    break
                self.samples.append((text, start_idx))
                sample_count += 1

            text_count += 1

        print(f"Dataset prêt: {len(self.samples)} samples depuis {text_count} textes")

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        text, start_idx = self.samples[idx]

        # Encoder SEULEMENT au moment de l'utilisation
        tokens = encode(text)

        # Extraire la séquence
        x = tokens[start_idx:start_idx + self.block_size]
        y = tokens[start_idx + 1:start_idx + self.block_size + 1]

        # Padding si nécessaire
        if len(x) < self.block_size:
            x = x + [0] * (self.block_size - len(x))
            y = y + [0] * (self.block_size - len(y))

        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)


# --- ALTERNATIVE: DATASET ULTRA-LÉGER (SI TROP DE RAM) ---
class MinimalStreamDataset(Dataset):
    """Version minimaliste: génère à la volée depuis le stream"""

    def __init__(self, stream_dataset, tokenizer, block_size, buffer_size=10000):
        self.tokenizer = tokenizer
        self.block_size = block_size
        self.buffer = []
        self.buffer_size = buffer_size

        # Remplir un buffer initial
        print(f"Remplissage du buffer ({buffer_size} samples)...")
        for i, item in enumerate(stream_dataset):
            if i >= buffer_size:
                break
            text = item["text"]
            if text and len(text.strip()) > 0:
                self.buffer.append(text)

        print(f"Buffer prêt: {len(self.buffer)} textes")

    def __len__(self):
        # Estimation approximative
        return len(self.buffer) * 10  # ~10 samples par texte en moyenne

    def __getitem__(self, idx):
        # Prendre un texte aléatoire du buffer
        text = random.choice(self.buffer)
        tokens = encode(text)

        if len(tokens) <= self.block_size:
            x = tokens + [0] * (self.block_size - len(tokens))
            y = x[1:] + [0]
        else:
            # Position aléatoire
            start = random.randint(0, len(tokens) - self.block_size - 1)
            x = tokens[start:start + self.block_size]
            y = tokens[start + 1:start + self.block_size + 1]

        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)


# --- CHARGEMENT DES DONNÉES EN STREAMING ---
print("Chargement du dataset en mode streaming...")

# MODE STREAMING : ne charge PAS tout en RAM
dataset_stream = load_dataset("rojagtap/bookcorpus", split="train", streaming=True)

# Échantillonner pour train/val sans tout charger
# On prend 1 texte sur 10 pour validation (10%)
train_texts_iter = dataset_stream.filter(lambda x, idx: idx % 10 != 0, with_indices=True)
val_texts_iter = dataset_stream.filter(lambda x, idx: idx % 10 == 0, with_indices=True)

print(f"Création des datasets en streaming...")
train_dataset = StreamingTextDataset(train_texts_iter, tokenizer, block_size, max_samples=100000)
val_dataset = StreamingTextDataset(val_texts_iter, tokenizer, block_size, max_samples=10000)

# DataLoaders (RÉDUIRE num_workers si RAM limitée)
train_loader = DataLoader(
    train_dataset,
    batch_size=batch_size,
    shuffle=True,
    num_workers=0,  # 0 = pas de multiprocessing (économise RAM)
    pin_memory=False  # Désactiver si pas de GPU
)

val_loader = DataLoader(
    val_dataset,
    batch_size=batch_size,
    shuffle=False,
    num_workers=0,
    pin_memory=False
)

print(f"Train samples: {len(train_dataset)}, Val samples: {len(val_dataset)}")


# --- 3. MODEL ---
class FeedForward(torch.nn.Module):
    def __init__(self, d_model):
        super().__init__()
        self.fc1 = torch.nn.Linear(d_model, 4*d_model)
        self.fc2 = torch.nn.Linear(4*d_model, d_model)
        self.relu = torch.nn.ReLU()
        self.drop = torch.nn.Dropout(dropout)

    def forward(self, x):
        return self.drop(self.fc2(self.relu(self.fc1(x))))


class MainLayer(torch.nn.Module):
    def __init__(self, d_model, num_heads):
        super().__init__()
        self.num_heads = num_heads
        self.multihead = torch.nn.MultiheadAttention(d_model, num_heads, batch_first=True)
        self.norm1 = torch.nn.LayerNorm(d_model)
        self.norm2 = torch.nn.LayerNorm(d_model)
        self.feed_forward = FeedForward(d_model)

    def compute_attn_mask(self, x):
        N, L = x.shape[:2]
        attn_mask = torch.triu(torch.ones(L, L, device=x.device), diagonal=1).bool()
        return attn_mask

    def forward(self, x):
        qkv = self.norm1(x)
        attn_output, _ = self.multihead(qkv, qkv, qkv, attn_mask=self.compute_attn_mask(x), need_weights=False)
        x = x + attn_output
        return x + self.feed_forward(self.norm2(x))


class TransformerDecoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.token_embedding = nn.Embedding(vocab_size, d_model)
        self.position_embedding = nn.Embedding(block_size, d_model)
        self.main_layers = torch.nn.Sequential(*[MainLayer(d_model, n_heads) for i in range(n_main_layers)])
        self.linear = nn.Linear(d_model, vocab_size)

    def forward(self, x):
        B, T = x.shape
        pos_emb = self.position_embedding(torch.arange(T, device=device))
        tok_emb = self.token_embedding(x)
        merged_embedding = tok_emb + pos_emb
        merged_embedding = self.main_layers(merged_embedding)
        return self.linear(merged_embedding)


def model_loss(model, x, targets):
    logits = model(x)
    B, T, V = logits.shape
    logits = logits.view(B * T, V)
    targets = targets.view(B * T)
    loss = F.cross_entropy(logits, targets)
    return loss


def generate(model, x, max_new_tokens):
    for _ in range(max_new_tokens):
        x_crop = x[:, -block_size:]
        logits = model(x_crop)
        logits = logits[:, -1, :]
        probs = F.softmax(logits, dim=1)
        x_next = torch.multinomial(probs, num_samples=1)
        x = torch.cat([x, x_next], dim=1)
    return x


@torch.no_grad()
def evaluate(model, dataloader, max_batches=None):
    model.eval()
    losses = []

    for i, (x, y) in enumerate(dataloader):
        if max_batches and i >= max_batches:
            break
        x, y = x.to(device), y.to(device)
        loss = model_loss(model, x, y)
        losses.append(loss.item())

    model.train()
    return np.mean(losses) if losses else 0.0


# --- 4. EXECUTION ---
model = TransformerDecoder()
model.load_state_dict(torch.load("lamilaz.pt", weights_only=True))
if torch.cuda.device_count() > 1:
    model = nn.DataParallel(model)
model = model.to(device)

print("Parameters:", sum(p.numel() for p in model.parameters())/1e6, 'M parameters')
optimizer = torch.optim.Adam(model.parameters(), lr=lr)

# BOUCLE D'ENTRAÎNEMENT
step = 0
for epoch in range(100):  # Nombre d'epochs arbitraire
    for x, y in train_loader:
        step += 1

        # Validation
        if step % eval_every == 0:
            train_loss = evaluate(model, train_loader, max_batches=eval_iters)
            val_loss = evaluate(model, val_loader, max_batches=eval_iters)
            prompts = torch.tensor([encode(p) for p in ["Hello", "Love", "Help"]]).to(device)
            outputs = generate(model, prompts, 100)
            outputs = [[decode(output.tolist())] for output in outputs]
            print(f"step {step}: train loss {train_loss:.4f}, val loss {val_loss:.4f}")

            wandb.log({
                "step": step,
                "train loss": train_loss,
                "validation loss": val_loss,
                "samples": wandb.Table(columns=["samples"], data=outputs)
            })

            # Test de génération
            try:
                start_ids = encode("Hello")
                if len(start_ids) > 0:
                    context = torch.tensor([start_ids], dtype=torch.long, device=device)
                    output = generate(model, context, max_new_tokens=50)
                    print(f"Generated: {decode(output[0].tolist())}")
            except Exception as e:
                print(f"Erreur generation: {e}")

        # Sauvegarde
        if step > 0 and step % save_every == 0:
            torch.save(model.state_dict(), checkpoint)

        # Train step
        x, y = x.to(device), y.to(device)
        optimizer.zero_grad()
        loss = model_loss(model, x, y)
        loss.backward()
        optimizer.step()

        if step >= max_iters:
            break

    if step >= max_iters:
        break

print("Training completed!")
wandb.finish()

import re
def gen(prompt, model=model, token=100):
    start_ids = tokenizer.encode(prompt).ids
    if len(start_ids) > 0:
        context = torch.tensor([start_ids], dtype=torch.long, device=device)
        output = generate(model, context, max_new_tokens=50)
        text = str(decode(output[0].tolist()))
        text = re.sub(r'§', ' ', re.sub(r' ', '', re.sub(r'  ', '§', text)))
        print(f"Generated: {text}")

gen("Nirvana")

